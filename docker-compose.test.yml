version: '3.8'

services:
  minio:
    image: minio/minio:latest
    container_name: autoloader-test-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    volumes:
      - minio_test_data:/data
    networks:
      - test_network

  postgres:
    image: postgres:15
    container_name: autoloader-test-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: autoloader_test
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d autoloader_test"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
    networks:
      - test_network

  spark-connect:
    image: apache/spark:3.5.0-scala2.12-java11-python3-ubuntu
    container_name: autoloader-test-spark-connect
    user: root
    ports:
      - "15002:15002"  # Spark Connect port
      - "4040:4040"    # Spark UI
    environment:
      # S3/MinIO Configuration
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_REGION=us-east-1
      - SPARK_NO_DAEMONIZE=true
    command: >
      bash -c "
      /opt/spark/sbin/start-connect-server.sh
      --properties-file /opt/spark/conf/spark-defaults.conf
      --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-connect_2.12:3.5.0
      && tail -f /opt/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-*.out
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4040"]
      interval: 30s
      timeout: 30s
      retries: 10
      start_period: 60s
    volumes:
      - spark_warehouse:/opt/spark/warehouse
      - spark_checkpoints:/tmp/checkpoints
      - spark_ivy_cache:/home/spark/.ivy2
      - ./docker-spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - test_network

  prefect-db:
    image: postgres:15
    container_name: autoloader-test-prefect-db
    environment:
      POSTGRES_DB: prefect
      POSTGRES_USER: prefect_user
      POSTGRES_PASSWORD: prefect_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U prefect_user -d prefect"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    volumes:
      - prefect_db_data:/var/lib/postgresql/data
    networks:
      - test_network

  prefect-server:
    image: prefecthq/prefect:3.5.0-python3.11
    container_name: autoloader-test-prefect
    ports:
      - "4200:4200"  # Prefect UI
    environment:
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://prefect_user:prefect_password@prefect-db:5432/prefect
      - PREFECT_API_URL=http://localhost:4200/api
    command: prefect server start --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4200/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      prefect-db:
        condition: service_healthy
    networks:
      - test_network

  prefect-worker:
    image: prefecthq/prefect:3.5.0-python3.11
    container_name: autoloader-test-prefect-worker
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      # Database connection for the worker (to access ingestion data)
      - DATABASE_URL=postgresql://test_user:test_password@postgres:5432/autoloader_test
      # MinIO/S3 access
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      # Spark Connect
      - SPARK_CONNECT_DEFAULT_PORT=15002
      # Python path to find app modules
      - PYTHONPATH=/opt/prefect
    command:
      - bash
      - -c
      - |
        echo 'Installing application dependencies...'
        pip install --quiet --no-cache-dir sqlalchemy==2.0.25 psycopg2-binary==2.9.9 "pydantic>=2.10.1,<3.0.0" pyspark==3.5.0 boto3==1.34.34 cryptography==41.0.7 python-dotenv==1.0.0 pydantic-settings pandas pyarrow
        echo 'Waiting for Prefect server to be ready...'
        sleep 10
        echo 'Creating work pool if it does not exist...'
        prefect work-pool create default-work-pool --type process || true
        echo 'Starting Prefect worker...'
        prefect worker start --pool default-work-pool
    volumes:
      # Mount application code so worker can execute flows
      - ./app:/opt/prefect/app:ro
      - ./tests:/opt/prefect/tests:ro
    working_dir: /opt/prefect
    depends_on:
      prefect-server:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      spark-connect:
        condition: service_healthy
    networks:
      - test_network
    restart: unless-stopped

volumes:
  minio_test_data:
    driver: local
  postgres_test_data:
    driver: local
  prefect_db_data:
    driver: local
  spark_warehouse:
    driver: local
  spark_checkpoints:
    driver: local
  spark_ivy_cache:
    driver: local

networks:
  test_network:
    driver: bridge
