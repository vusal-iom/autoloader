"""
Integration tests for BatchFileProcessor._process_single_file.
"""
import pytest
import json
import uuid
import io
from pyspark.sql.types import StructType, StructField, LongType, StringType
from chispa.dataframe_comparer import assert_df_equality

from app.services.batch_file_processor import BatchFileProcessor
from app.services.file_state_service import FileStateService
from app.repositories.ingestion_repository import IngestionRepository
from app.models.domain import Ingestion, IngestionStatus
from tests.helpers.logger import TestLogger

@pytest.mark.integration
class TestBatchFileProcessor:
    """Test BatchFileProcessor._process_single_file method."""

    @pytest.fixture
    def ingestion(self, test_db):
        """Create test ingestion."""
        repo = IngestionRepository(test_db)
        unique_id = str(uuid.uuid4())
        ingestion = Ingestion(
            id=f"test-ingestion-{unique_id}",
            tenant_id="test-tenant",
            name="Test Batch Processor",
            cluster_id="test-cluster-1",
            source_type="s3",
            source_path="s3://test-bucket/data/",
            source_credentials={
                "aws_access_key_id": "test",
                "aws_secret_access_key": "test"
            },
            format_type="json",
            destination_catalog="test_catalog",
            destination_database="test_db",
            destination_table=f"batch_test_{unique_id[:8]}",
            checkpoint_location=f"/tmp/test-checkpoint-{unique_id}",
            status=IngestionStatus.ACTIVE,
            on_schema_change="append_new_columns",
            schema_version=1,
            created_by="test-user"
        )
        test_db.add(ingestion)
        test_db.commit()
        test_db.refresh(ingestion)
        return ingestion

    def _upload_file(self, minio_client, bucket, key, content):
        """Helper to upload file to MinIO."""
        data = json.dumps(content).encode('utf-8')
        minio_client.put_object(
            Bucket=bucket,
            Key=key,
            Body=io.BytesIO(data)
        )
        return f"s3a://{bucket}/{key}"

    def test_process_single_file_success_infer_schema(
        self, test_db, spark_client, spark_session, minio_client, lakehouse_bucket, ingestion, temporary_table
    ):
        """Test processing a single file with schema inference."""
        logger = TestLogger()
        logger.section("Integration Test: Process Single File (Infer Schema)")

        # 1. Setup: Upload file
        logger.phase("Setup: Upload test file")
        file_data = [
            {"id": 1, "name": "Alice", "score": 95},
            {"id": 2, "name": "Bob", "score": 88}
        ]
        file_key = f"data/test_infer_{uuid.uuid4()}.json"
        s3_path = self._upload_file(minio_client, lakehouse_bucket, file_key, file_data)
        logger.step(f"Uploaded file to {s3_path}")

        # Use temporary_table fixture to manage table lifecycle
        # We need to override the ingestion's table name to use the one generated by the fixture
        table_id = temporary_table(prefix="batch_infer", logger=logger)
        ingestion.destination_table = table_id.split(".")[-1]
        test_db.commit()
        logger.step(f"Configured ingestion to use table {table_id}")

        # 2. Action
        logger.phase("Action: Process file")
        state_service = FileStateService(test_db)
        processor = BatchFileProcessor(spark_client, state_service, ingestion, test_db)
        
        file_info = {"path": s3_path, "size": 100, "last_modified": 1234567890}
        result = processor._process_single_file(s3_path, file_info)
        logger.step("Processed file")

        # 3. Verification
        logger.phase("Verify: Check record count and table content")
        assert result['record_count'] == 2
        
        df_actual = spark_session.table(table_id).orderBy("id")
        
        # Explicit schema for expected data
        expected_schema = StructType([
            StructField("id", LongType(), True),
            StructField("name", StringType(), True),
            StructField("score", LongType(), True)
        ])
        
        df_expected = spark_session.createDataFrame(file_data, schema=expected_schema)
        
        assert_df_equality(
            df1=df_actual,
            df2=df_expected,
            ignore_column_order=True,
            ignore_row_order=True
        )
        logger.success("Table content matches expected data")

    def test_process_single_file_success_predefined_schema(
        self, test_db, spark_client, spark_session, minio_client, lakehouse_bucket, ingestion, temporary_table
    ):
        """Test processing a single file with predefined schema."""
        logger = TestLogger()
        logger.section("Integration Test: Process Single File (Predefined Schema)")

        # 1. Setup: Configure schema
        logger.phase("Setup: Configure schema and upload file")
        schema = {
            "type": "struct",
            "fields": [
                {"name": "id", "type": "long", "nullable": True, "metadata": {}},
                {"name": "name", "type": "string", "nullable": True, "metadata": {}}
            ]
        }
        ingestion.schema_json = json.dumps(schema)
        
        table_id = temporary_table(prefix="batch_schema", logger=logger)
        ingestion.destination_table = table_id.split(".")[-1]
        test_db.commit()

        file_data = [
            {"id": 1, "name": "Alice", "score": 95},
            {"id": 2, "name": "Bob", "score": 88}
        ]
        file_key = f"data/test_schema_{uuid.uuid4()}.json"
        s3_path = self._upload_file(minio_client, lakehouse_bucket, file_key, file_data)
        logger.step(f"Uploaded file with extra 'score' field to {s3_path}")

        # 2. Action
        logger.phase("Action: Process file")
        state_service = FileStateService(test_db)
        processor = BatchFileProcessor(spark_client, state_service, ingestion, test_db)
        
        file_info = {"path": s3_path, "size": 100}
        result = processor._process_single_file(s3_path, file_info)
        logger.step("Processed file")

        # 3. Verification
        logger.phase("Verify: Check schema enforcement")
        assert result['record_count'] == 2
        
        df_actual = spark_session.table(table_id).orderBy("id")
        
        # Expected schema should NOT have 'score'
        expected_schema = StructType([
            StructField("id", LongType(), True),
            StructField("name", StringType(), True)
        ])
        
        expected_data = [
            {"id": 1, "name": "Alice"},
            {"id": 2, "name": "Bob"}
        ]
        df_expected = spark_session.createDataFrame(expected_data, schema=expected_schema)
        
        assert_df_equality(
            df1=df_actual,
            df2=df_expected,
            ignore_column_order=True,
            ignore_row_order=True
        )
        logger.success("Table content adheres to strict schema (extra column ignored)")

    def test_process_single_file_empty_file(
        self, test_db, spark_client, spark_session, minio_client, lakehouse_bucket, ingestion, temporary_table
    ):
        """Test processing an empty file."""
        logger = TestLogger()
        logger.section("Integration Test: Process Empty File")

        # 1. Setup: Upload empty list JSON
        logger.phase("Setup: Upload empty file")
        file_data = []
        file_key = f"data/test_empty_{uuid.uuid4()}.json"
        s3_path = self._upload_file(minio_client, lakehouse_bucket, file_key, file_data)
        
        table_id = temporary_table(prefix="batch_empty", logger=logger)
        ingestion.destination_table = table_id.split(".")[-1]
        test_db.commit()

        # 2. Action
        logger.phase("Action: Process file")
        state_service = FileStateService(test_db)
        processor = BatchFileProcessor(spark_client, state_service, ingestion, test_db)
        
        file_info = {"path": s3_path, "size": 10}
        result = processor._process_single_file(s3_path, file_info)
        logger.step("Processed file")

        # 3. Verification
        logger.phase("Verify: No data written")
        assert result['record_count'] == 0
        
        # Table should exist (created by fixture) but be empty? 
        # Actually, temporary_table fixture doesn't CREATE the table, it just generates a name and handles DROP.
        # The code creates the table on first write.
        # Since record_count is 0, _write_to_iceberg is skipped, so table should NOT exist.
        
        table_exists = spark_session.catalog.tableExists(table_id)
        assert not table_exists
        logger.success("Table was not created")

    def test_process_single_file_malformed_file(
        self, test_db, spark_client, spark_session, minio_client, lakehouse_bucket, ingestion
    ):
        """Test processing a malformed file."""
        logger = TestLogger()
        logger.section("Integration Test: Process Malformed File")

        # 1. Setup: Upload malformed content
        logger.phase("Setup: Upload malformed file")
        content = b"{ this is not valid json }"
        file_key = f"data/test_malformed_{uuid.uuid4()}.json"
        minio_client.put_object(
            Bucket=lakehouse_bucket,
            Key=file_key,
            Body=io.BytesIO(content)
        )
        s3_path = f"s3a://{lakehouse_bucket}/{file_key}"
        
        ingestion.format_options = json.dumps({"mode": "FAILFAST"})
        test_db.commit()

        # 2. Action & Verification
        logger.phase("Action: Process file and expect failure")
        state_service = FileStateService(test_db)
        processor = BatchFileProcessor(spark_client, state_service, ingestion, test_db)
        
        file_info = {"path": s3_path, "size": 100}
        
        with pytest.raises(Exception):
            processor._process_single_file(s3_path, file_info)
        logger.success("Exception raised as expected")

    def test_process_single_file_schema_evolution(
        self, test_db, spark_client, spark_session, minio_client, lakehouse_bucket, ingestion, temporary_table
    ):
        """Test schema evolution across two files."""
        logger = TestLogger()
        logger.section("Integration Test: Schema Evolution")

        # 1. Setup: File 1
        logger.phase("Setup: Process first file")
        file1_data = [{"id": 1, "name": "Alice"}]
        file1_key = f"data/test_evolve_1_{uuid.uuid4()}.json"
        s3_path1 = self._upload_file(minio_client, lakehouse_bucket, file1_key, file1_data)
        
        table_id = temporary_table(prefix="batch_evolve", logger=logger)
        ingestion.destination_table = table_id.split(".")[-1]
        test_db.commit()
        
        state_service = FileStateService(test_db)
        processor = BatchFileProcessor(spark_client, state_service, ingestion, test_db)

        processor._process_single_file(s3_path1, {"path": s3_path1, "size": 100})
        logger.step("Processed file 1")
        
        # 2. Setup: File 2 with NEW column
        logger.phase("Action: Process second file with new column")
        file2_data = [{"id": 2, "name": "Bob", "email": "bob@example.com"}]
        file2_key = f"data/test_evolve_2_{uuid.uuid4()}.json"
        s3_path2 = self._upload_file(minio_client, lakehouse_bucket, file2_key, file2_data)
        
        processor._process_single_file(s3_path2, {"path": s3_path2, "size": 100})
        logger.step("Processed file 2")
        
        # 3. Verification
        logger.phase("Verify: Check evolved schema and data")
        df_actual = spark_session.table(table_id).orderBy("id")
        
        expected_schema = StructType([
            StructField("id", LongType(), True),
            StructField("name", StringType(), True),
            StructField("email", StringType(), True)
        ])
        
        expected_data = [
            {"id": 1, "name": "Alice", "email": None},
            {"id": 2, "name": "Bob", "email": "bob@example.com"}
        ]
        df_expected = spark_session.createDataFrame(expected_data, schema=expected_schema)
        
        assert_df_equality(
            df1=df_actual,
            df2=df_expected,
            ignore_column_order=True,
            ignore_row_order=True
        )
        logger.success("Table evolved correctly and contains all data")
